---
title: RAG
description: |-
  User Query -> Query Classification -> Retrieval -> Reranking -> Repacking -> Summarization -> Large Language Model -> Generate Response
  Retrieval Source
  Docu...
id: C138
"no": 138
order: 138
category: ai
subCategory: machine-learning
tags:
  - RAG
  - 검색증강생성
  - AI
  - LLM
  - retrieval
created: "2025-04-07T06:47:51.946Z"
updated: "2025-04-10T01:32:29.364Z"
published: null
featured: false
draft: true
hasChildren: false
elementCount: 80
childCount: 0
slug: rag
---

# RAG Flow

- User Query -> Query Classification -> Retrieval -> Reranking -> Repacking -> Summarization -> Large Language Model -> Generate Response
- Retrieval Source
  - Documents -> Chunking -> Embedding -> Vector Database
- 필요한 경우
  - Fine-tune
  - Evaluation
![8a43bad1-0164-4f73-a17c-3c8ebcd93bed](https://miro.medium.com/v2/resize:fit:1260/0*p_LnCvVGjvtrriKF.png)



# Best Practices of Each Steps



## 1. Query Classification(쿼리 분류)

- 유저 쿼리(질문)가 검색이 필요한지 판단
- LLM이 자체적으로 처리할 수 있는 쿼리는 검색 없이 응답하는 것이 효율적
  - ex) 단순계산, 설명 요청('GPT 모델 종류는?')


## 2. Chunking(청킹)

- 문서를 더 작은 청크로 나누는 것
- 청킹은 검색 정확도와 LLM의 입력 길이에 영향을 줌
- 방법
  - Token-level Chunking(토큰 단위 청킹)
    - 간단하지만 문장이 분할되어 문맥이 깨질 수 있음 -> 검색 품질에 영향
  - Semantic-level Chunking(의미 단위 청킹)
    - LLM을 사용하여 의미 있는 단위로 나눔 -> 문맥을 유지하지만 더 많은 시간 소요
  - **Sentence-level Chunking(문장 단위 청킹)**
    - 문맥을 유지하고 효율적
- 평가 기준
  - Chunking Size
  - Chunking Techniques
  - Embedding Model Selection
  - Metadata Addition


## 3. Embedding



## 4. Vector Database



## 5. Retrieval(검색)

- 유저 쿼리와 관련성이 높은 상위 N개 문서를 선택
- Query Rewriting
  - LLM이 쿼리를 개선하여 재작성


## 6. Reranking(재정렬)

- 검색된 문서를 쿼리와 가장 관련성 있는 순서대로 정렬
- DLM(Deep Language Model) Reranking
  - 재정렬을 위해 심층 언어 모델(DLM)을 사용
  - DLM은 문서의 쿼리 관련성을 "true" 또는 "false"으로 분류하도록 미세 조정됨
  - monoT5
  - monoBERT
  - RankLLaMA
- TILDE Reranking


## 7. Repacking(재구성)

- 문서 순서에 따라 LLM의 응답 생성에 영향을 줄 수 있음
- "forward"
  - Reranking 단계의 관련성 순서에 따라 내림차순으로 구성
- "reverse"
  - Reranking 단계의 관련성 순서에 따라 오름차순으로 구성
- **"sides"**
  - 관련 문서를 입력의 시작 또는 끝에 배치하며 Lost in the Middles에 효과적


## 8. Summarization(요약)

- 검색된 문서를 요약하는 것은 RAG 프로세스에서 매우 중요함
- 검색 결과의 중복되거나 불필요한 정보는 LLM의 정확한 응답 생성을 방해
- 긴 프롬프트는 LLM의 추론 프로세스를 느리게 함
- Selective Context
  - 입력 context에서 중복된 정보를 제거


## 9. Generator Fine-tuning(미세 조정)

- 관련 문서와 비관련 문서를 혼합한 데이터를 학습하여 LLM의 성능 향상
